# 시간복잡도

### 시간복잡도를 구하기 전에...
HW/SW 환경이 사용자마다 다르기 때문에 자료구조와 알고리즘의 성능을 계산하기가 어렵다.
다양한 크기의 입력에 대해 내가 작성한 코드가 어느정도의 시간이 필요한지 계산하기 위해서는 객관적인 컴퓨터 모델이 필요하다.

그래서 가상컴퓨터(Virtual Machine) + 가상언어(Pseudo Language) + 가상코드(Pseudo Code)를 가정한다.

**가상 환경**
- 가상컴퓨터 : 원시적인 형태의 컴퓨터 모델인 Turing Machine이 현대적인 모델로 정착한건 폰노이만이라는 수학자가 제시한 모델인 RAM(Random Access Machine)으로부터다.
RAM은 CPU, Memory, 기본연산으로 구성된다

- 기본연산 : 단위 시간에 수행되는 연산이다. (시간 단위를 1로 계산한다)
> 배정, 대입, 복사연산 : A = B
산술연산 : +, -, *, /
비교연산 : >, ==, < ....
논리연산 : AND, OR, NOT
비트연산 : bit - AND, OR, NOT
 -> 이러한 기본 연산으로 정리한 가상의 컴퓨터를 RAM모델이라고 부르고 RAM모델을 통해 알고리즘 시간을 측정할 수 있다.

- 가상언어 : 
1. 배정연산, 산술연산, 비교연산, 논리연산, bit논리연산을 표현 할 수 있어야한다.
2. 비교문 if, else를 사용 할 수 있어야한다.
3. 반복문 for, while을 사용할 수 있어야한다.
4. 함수를 정의, 호출, return이 가능해야 한다.
RAM에서 동작 가능한 위의 4가지 조건이 가능하면 모두 가상언어가 될 수 있다.
이를 통해 아래와 같이 작성된 코드를 가상코드(Pseudo Code)라 칭한다.
```
algorithm ArrayMax(A, n) :
	#input : n개의 정수를 갖는 배열 A
    #output : A의 수 중에서 최대값을 찾아 리턴
    currentMax = A[0]
    for i = 1 to n-1 do
    	if currentMax < A[i] :
        	currentMax = A[i]
   return currentMax
```


### 가상 코드의 시간 복잡도 계산해보기
위의 가상 코드를 통해 시간 복잡도를 계산해보기.
- 파라미터 정의
A = [3, -1, 9, 2, 12] , n = 5

- 기본 연산 계산 방법
위의 파라미터에 대한 기본 연산은 총 7회이다.
하지만 무한히 많은 크기의 A와 입력의 크기가 무한한 n일때 매번 이렇게 측정하기는 힘들다

- 그럼 어떻게?
1. 모든 입력에 대해 기본연산 횟수를 더한 후 평균을 낸다. 
	-> 모든 알고리즘에 대해 고려해야 될 입력이 무한하기 때문에 불가능하다
2.  가장 안좋은 입력(Worstcase Input)에 대한 기본 연산 횟수를 측정한다. = Worstcase time complexity
-> 가장 안좋은 입력이라는 것은 알고리즘의 기본연산 횟수를 최대한 많이 필요하게 만드는 입력이다. 어떤 입력에 대해서도 WTC보다 수행시간이 크지 않다. 
즉, 알고리즘의 수행시간은 최악의 입력에 대한 기본 연산 횟수로 정의한다.

- 적용해보기
```
sum1(A, n) :
	sum = 0 # 1
    for i = 0 to n-1 do # n
    	if A[i] % 2 == 0 : # 2
        	sum += A[i]    # 2
    return sum
    # 모든 값이 짝수인 경우 
    # T(n) = 4n + 1
```
```
sum2(A,n) :
	sum = 0 #1
    for i = 0 to n-1 do # n
    	for j = i to n-1 do # 
        	sum += A[i] * A[j] # 3
	return sum
    # 분기처리가 되지 않기 때문에 모든 입력이 다 최악의 경우다
    # T(n) = 3/2(n*n)-3/2n+1
```

### Big - O 표기법 
알고리즘의 수행시간은 최악의 경우의 입력에 대한 기본 연산 횟수이다.
위와 같이 모든 항들을 계산해서 표기하는 것이 아닌, n값에 대한 최고 차항을 간단하게 표기하는 것이다.

위의 ArrayMax와 sum1은 n값이 선형적으로 증가 할 수록 수행시간이 증가한다.
즉 O(n)으로 표현할 수 있다.
하지만 sum2는 n의 값이 증가함에 따라 수행시간이 제곱으로 증가한다.
즉 O(n*n)로 표기할 수 있다.

1. 최고차항만 남긴다.
2. 최고차항 계수(상수)는 생략한다.
3. Big-O로 감싼다 O(최고차항)

**집합으로 이해하기**
T1(n) = O(n)
T2(n) = O(n)
T3(n) = T1(n) , T2(n)
O(n)이라는 집합안에 T1과 T2가 원소로 존재한다.
T3(n)은 O(n*n)의 집합안에 원소로 존재하고
O(n)은 O(n*n)의 집합안에 존재한다.

**코드예시**
```
def increment_one(a) :
	return a+1
    # 상수항만 존재한다.
    # T(n) = 1
    # O(n) = O(1)
```

```
def number_of_bits(n) :
	count = 0
    while n > 0 :
    	n = n//2
        count += 1
    return count
    # n/2 -> n/(2*2) ->... ->  n/(2의 count제곱)
    # n/2의 count제곱 = 1
    # n = 2의 count제곱
    # log2의 n제곱 = count
    # O(log2의 n제곱)
```

즉 시간복잡도의 관계는 
O(1) > O(log2의n제곱) > O(n) > O(n의2제곱)
왼쪽이 제일 빠르며 오른쪽에 속해있다.